{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web_Scraping using python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective :\n",
    "\n",
    "The objective of this project is to automate the extraction of data from websites. Web scraping can help transform unstructured web data into structured datasets that are easier to analyze and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Liraries :\n",
    "\n",
    "We import the 'requests' library to make HTTP requests and interact with web services and import 'BeautifulSoup' library to parse HTML and XML documents.\n",
    "\n",
    "BeautifulSoup library is used to managing hierarchical data structures which simplifies the process of web scraping by converting complex HTML documents into a format that is easy to work with python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging:\n",
    "\n",
    " The logging module in Python provides a flexible framework for emitting log messages from Python programs. The logging.basicConfig function is used to configure the logging system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, \n",
    "                    format='%(asctime)s %(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to scrape data from a given URL\n",
    "**scraped_data** is designed to fetch and parse HTML content from the given urls.\n",
    "\n",
    "Create a try_except block for fetching urls,response.raise_for_status() checks if the request was successful (status code 200). If not, it raises an HTTPError.\n",
    "\n",
    "Then using BeautifulSoup library extract the data from tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_data(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        tables = soup.find_all('table', class_='table')\n",
    "        data_frames = []\n",
    "        \n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "            data = []\n",
    "            for row in rows[1:]:\n",
    "                cols = row.find_all('td')\n",
    "                data.append([col.text.strip() for col in cols])\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            data_frames.append(df)\n",
    "        \n",
    "        return data_frames\n",
    "    except AttributeError as e:\n",
    "        logging.error(f\"Error parsing HTML from {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save DataFrame to PostgreSQL:\n",
    "Create save_to_postgresql funtion to save dataframe to postgresql database.\n",
    "\n",
    "The **create_engine** function is used to create an Engine object, which serves as the primary interface to the database.The Engine manages connections to the database and provides a high-level interface for executing SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_postgresql(df, table_name, username, password, host, dbname):\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql+psycopg2://{username}:{password}@{host}/{dbname}')\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        logging.info(f\"Data saved to PostgreSQL table {table_name} successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to PostgreSQL table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetched the URLs from the Website\n",
    "\n",
    "Website link: https://www.scrapethissite.com/pages/ajax-javascript/#2015\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/\n",
    "\n",
    "https://www.scrapethissite.com/pages/advanced/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the database connection details and the base table name.\n",
    "\n",
    "Call the main function to scrape the data from the URLs and save it to the PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    urls = [\n",
    "        \"https://www.scrapethissite.com/pages/ajax-javascript/#2015\",\n",
    "        \"https://www.scrapethissite.com/pages/forms/\",\n",
    "        \"https://www.scrapethissite.com/pages/advanced/\"\n",
    "    ]\n",
    "    \n",
    "    username = 'postgres'\n",
    "    password = 'sql123'\n",
    "    host = 'localhost'\n",
    "    dbname = 'web_scraping'\n",
    "    table_name = 'scraped_data'\n",
    "\n",
    "    for url in urls:\n",
    "        data_frames = scrape_data(url)\n",
    "        if data_frames:\n",
    "            for idx, df in enumerate(data_frames):\n",
    "                table_name = f\"team_data_{idx}\"\n",
    "                save_to_postgresql(df, table_name, username, password, host, dbname)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraped_data is saved to the postgresql database web_scraping.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
